{"cells":[{"cell_type":"code","source":["!pip -q install openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-H5b9qWJiHc","executionInfo":{"status":"ok","timestamp":1717518221493,"user_tz":-600,"elapsed":20789,"user":{"displayName":"Trung Nguyễn Vũ","userId":"02158753298940026143"}},"outputId":"2605cf6a-438b-4bb7-a224-e26de51d3303"},"id":"p-H5b9qWJiHc","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guyI0hK3JizW","executionInfo":{"status":"ok","timestamp":1717518245766,"user_tz":-600,"elapsed":24280,"user":{"displayName":"Trung Nguyễn Vũ","userId":"02158753298940026143"}},"outputId":"fc2c84a0-d408-419a-c533-066ebf4a795f"},"id":"guyI0hK3JizW","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import requests\n","import http,json\n","import openai\n","import os\n","import pandas as pd\n","from datetime import datetime\n","\n","# import local library|\n","import sys\n","# sys.path.append(r'/content/drive/Othercomputers/My Laptop/NIT6001/CMG_prog_v2')\n","sys.path.append(os.path.abspath('/content/drive/Othercomputers/My Laptop/NIT6001/CMG_prog_v2'))\n","from GPTCall import ask_chatgpt\n","from SaveExcel_v2 import update_sheet_preserving_format, update_list_sheets_preserving_format"],"metadata":{"id":"RcNGo0hgJZQ9","executionInfo":{"status":"ok","timestamp":1717518250257,"user_tz":-600,"elapsed":4495,"user":{"displayName":"Trung Nguyễn Vũ","userId":"02158753298940026143"}}},"id":"RcNGo0hgJZQ9","execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"id":"afa08f17-1b52-4de2-bc22-b3b8d65b9118","metadata":{"tags":[],"id":"afa08f17-1b52-4de2-bc22-b3b8d65b9118","executionInfo":{"status":"ok","timestamp":1717518250258,"user_tz":-600,"elapsed":4,"user":{"displayName":"Trung Nguyễn Vũ","userId":"02158753298940026143"}}},"outputs":[],"source":["mypath = r\"/content/drive/Othercomputers/My Laptop/NIT6001/CMG_dataset\"\n","myfile = r\"/MSDManual/Cognitive Map Graph Processing v4 2024.03.14.xlsx\"\n","process_knowledge_filename = r\"/MSDManual/CMG_article_process_knowledge.xlsx\"\n","\n","# Use os.path.join to create the full paths\n","process_knowledge_file_fullpath = os.path.join(mypath, process_knowledge_filename.lstrip('/'))\n","myexcelfile = os.path.join(mypath, myfile.lstrip('/'))"]},{"cell_type":"code","execution_count":5,"id":"000f7958-ba76-4944-8456-03d9de10e5fe","metadata":{"tags":[],"id":"000f7958-ba76-4944-8456-03d9de10e5fe","executionInfo":{"status":"ok","timestamp":1717518250258,"user_tz":-600,"elapsed":3,"user":{"displayName":"Trung Nguyễn Vũ","userId":"02158753298940026143"}}},"outputs":[],"source":["import time\n","def keyfunction_readme():\n","    i=i\n","    # key function steps\n","    #-------------------------------------\n","\n","    # 1.  In mypath folder, there is an excel file called Cognitive Map Graph Processing v3 2024.02.14.xlsx\n","    # 2.  The excel file hastwo sheets, one are paragraphs, and one are cognitive map graph sentences\n","        # Sheet name: paragraphs\n","        #Columns: ['ID', 'Paragraph text', 'url', 'category labels', 'summarised key points in simple sentences', 'processing user', 'processing date']\n","\n","        #Sheet name: sentences\n","        #Columns: ['ID', 'paragraph ID', 'CMG Auto with GPT', 'CMG by Human Expert', 'Justification of the correction', 'processing user', 'processing date', 'correction user', 'corrction date']\n","\n","    # 3. Read the original text to a dataframe called df, run through it row by row, call ChatGPT API,\n","    #     use the following myprompt to summarise the key points of the text:\n","    #     myprompt=\"1) Summarise the key point, or information/knowledge, of the following text,\n","    #               2) use simple structrued setnecnes;  3) each sentence should be self contained, avoid using propositions\n","    #               to refer to entities in ealrier sentences; 4) response in format of  Key Points =  'the key points' \"\n","    # 4. Parse the ChatGPT response to extract the keypoints, and update the keypoints in col4 of the dataframe df\n","    # 5. For each row in col4, ask chatGPT API to convert the sentences into  head, relation, tail structure. For example,\n","    #        Acute kidney injury is a rapid decrease in renal function over days to weeks. will be separated into:\n","    #        Acute kidney injury, is, a rapid decrease in renal function (duration: over days to weeks).\n","    #     Here we use () to enclose properties of the head, tail or relation. Multiple properties can be separated with comma.\n","    # 6. Note that a sentence may not have a tail, which can be represented with a -. For example,\n","    #       Acute kidney injury can be fatal.   can be converted as\n","    #       Acute kidney injury, can be fatal, -.\n","    # 7. For a sentence with a sub clause, use [] to enclose the main sentence and the sub clause. Use []-(connecting word)-[]. for the converted sentence.\n","    #      for example,  Tom had AKI when he was 50.  will be converted as\n","    #                   [Tom, had AKI, -]-(when)-[Tom, was 50, -]\n","    #      note the relationship needs to be meaningful. is, have, get are too short to represent the meaning of the relation.\n","    # 8. Resonse will be in format of  FCM scripts= ' ****'\n","    # 9. Extract FCM scripts from the response, and write to col5 of df\n","\n","def main():\n","    print (\"main function started \\n--------------------\")\n","    time_started=time.time()\n","\n","    # myexcelfile=mypath+'Cognitive Map Graph Processing v4 2024.03.14.xlsx'\n","    #check_excelfile_info(myexcelfile)\n","\n","    df_paragraphs = pd.read_excel(myexcelfile, sheet_name='paragraphs')\n","    df_sentences = pd.read_excel(myexcelfile, sheet_name='sentences')\n","\n","    row_start=0;    row_end=0 # end is 0 means to the end\n","    df_paragraphs, df_sentences=summarise_keypoints(df_paragraphs,  df_sentences, row_start, row_end)\n","\n","    update_sheet_preserving_format(myexcelfile, 'paragraphs', df_paragraphs)\n","    update_sheet_preserving_format(myexcelfile, 'sentences', df_sentences)\n","\n","    time_finished=time.time()\n","    timeused=time_finished-time_started\n","    print(\"Time used=\", round(timeused,2))\n","\n","\n","def check_excelfile_info(myexcelfile):\n","# check the sheet names and columns in the excel file\n","     # Iterate through all sheets\n","    print(myexcelfile)\n","    xls = pd.ExcelFile(myexcelfile)\n","\n","    for sheet_name in xls.sheet_names:\n","        # Read each sheet\n","        df = pd.read_excel(xls, sheet_name)\n","\n","        # Print the sheet name and its columns\n","        print(f\"Sheet name: {sheet_name}\")\n","        print(\"Columns:\", df.columns.tolist())\n","\n","def summarise_prompt():\n","    myprompt1=\"\"\" Summarise the following paragraph's key points and knowledge points.\n","  1) Use simple structured sentences when possible. A simple structured sentence is not a simple sentence. We only need to minimize the usage of complex structures.\n","  2) The answer contains the key points and knowledge points only. Do not add explanations.\n","  3) Do not use pronouns. Use the proper nouns because the sentences will be further processed individually. Sometimes, you need to retrieve from the context to replace the nouns so that the sentences in the summary will be self-contained. It can be used individually without needing the context.\n","  4) Each sentence is one line. Each sentence will be processed independently. So, try to keep the meaning of the sentence self-contained.\n","  5) If the original sentence is already a simple sentence, you can use the same sentence.\n","  6) Use the same language as the paragraph content.\n","\n","  7) Example\n","Here is a summarised keypoints of a paragraph.\n","\"….   - Stereotactic deep brain stimulation or lesional surgery may assist patients with refractory symptoms who do not have dementia.\n","- An apomorphine pump can also be used for Parkinson disease treatment.\n","- The paragraph references an Overview of Movement and Cerebellar Disorders for additional information.\"\n","\n","The last sentence can be converted to:  Parkinson disease knowledge is also related to the section of movement and cerebellar disorders in MSD manual.\n","\n","In this case, we added the missing key information so that the sentence carries the knowledge that is not dependent on the other sentences.\n","\n","\n","  Here is the paragraph: \"\"\"\n","    df = pd.read_excel(process_knowledge_file_fullpath, sheet_name=\"knowledge\", engine='openpyxl')\n","    filtered_df = df[(df['knowledge_area'] == \"step2_summarise_paragraphs\") ]\n","    if filtered_df.empty:\n","        myprompt=\"Could not read the knowledge on step2_summarise_paragraphs.\\n\"\n","    else:\n","        myprompt = '\\n'.join(filtered_df['knowledge'].astype(str))\n","    return myprompt +\"\\n Here is the paragraph:\"\n","\n","\n","def summarise_keypoints(df, df_sentences,row_start, row_end):\n","    print(\"\\n summarise_keypoints function \\n --------------------------------------\")\n","    # print(df_sentences)\n","    myprompt=summarise_prompt()\n","\n","    if row_end == 0:   row_end = df.index[-1]\n","    for index in range(row_start, row_end + 1):\n","        if index > df.index[-1]:  # Check to ensure index is within DataFrame bounds\n","            break  # Exit the loop if index exceeds the number of rows in the DataFrame\n","\n","        # Access row by index\n","        row = df.iloc[index]\n","        mycontent = row['Paragraph text']\n","        summary = row['summarised key points in simple sentences']\n","        processedflag=row['processed']\n","\n","        # Proceed if 'Paragraph text' is not empty and processed is not 'Yes'\n","        if processedflag!='Yes' and pd.notna(mycontent)  and mycontent.strip():\n","            response_text = ask_chatgpt(myprompt, mycontent)\n","            print(\"-------response_text-----------------------\")\n","            print(response_text)\n","\n","            # Update the DataFrame with the response\n","            df.at[index, 'summarised key points in simple sentences'] = response_text\n","            paragraph=response_text\n","            df_sentences=write_summarisedPoints_to_sentence_rows(paragraph,index, df_sentences)\n","            df.at[index,'processed']='Yes'\n","\n","    return df,df_sentences\n","\n","\n","def write_summarisedPoints_to_sentence_rows(paragraph,index, df_sentences):\n","    print (\"\\n split_sentences function index=\",index,\" \\n -------------------------------------\")\n","    print(type(df_sentences))\n","    print(df_sentences)\n","    if not df_sentences.empty:\n","        # If df_sentences is not empty, continue IDs from the last used ID\n","        sentence_id = df_sentences['Sentence ID'].max() + 1\n","    else:  sentence_id = 1  # If df_sentences is empty, start IDs from 1\n","\n","    new_rows = []  # Initialize a list to hold new rows\n","    paragraph_id = index+1\n","    # summarised_sentences = paragraph.split('\\n')\n","\n","    # for sentence in summarised_sentences:\n","    #         #print(\"\\n here=\", sentence)\n","    #         if sentence and sentence.strip():  # Check if the sentence is not just whitespace\n","    #             # Create a new row with existing columns, setting default values for unspecified columns\n","    #             new_row = {col: '' for col in df_sentences.columns}  # Initialize all columns to default values\n","    #             new_row.update({\n","    #                 'Sentence ID': sentence_id,\n","    #                 'Paragraph ID': paragraph_id,\n","    #                 'Sentence text': sentence.strip()\n","    #             })\n","    #             new_rows.append(new_row)\n","    #             sentence_id += 1\n","\n","    new_row = {col: '' for col in df_sentences.columns}\n","    new_row.update({\n","        'Sentence ID': sentence_id,\n","        'Paragraph ID': paragraph_id,\n","        'Sentence text': paragraph\n","    })\n","    new_rows.append(new_row)\n","    sentence_id += 1\n","    # Append new rows to df_sentences DataFrame\n","    if new_rows:\n","        df_sentences = pd.concat([df_sentences, pd.DataFrame(new_rows)], ignore_index=True)\n","\n","    return df_sentences"]},{"cell_type":"code","execution_count":6,"id":"d2e7cd35-f174-4af1-92f5-e4048b7f63ac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2e7cd35-f174-4af1-92f5-e4048b7f63ac","executionInfo":{"status":"ok","timestamp":1717518261390,"user_tz":-600,"elapsed":11135,"user":{"displayName":"Trung Nguyễn Vũ","userId":"02158753298940026143"}},"outputId":"cd4b18af-52c0-4df2-c8e0-d7ffe94f9e0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["main function started \n","--------------------\n","\n"," summarise_keypoints function \n"," --------------------------------------\n","Time used= 11.2\n"]}],"source":["main()"]}],"metadata":{"kernelspec":{"display_name":"Python (NIT003)","language":"python","name":"nit003"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}